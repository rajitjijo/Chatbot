{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fce05fce-a1fd-49a4-972d-46860777d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Vocabulary import vocabulary\n",
    "from preprocessing import normalizeString, unicodetoascii\n",
    "import itertools\n",
    "import torch\n",
    "import random\n",
    "from model import *\n",
    "from NLLLoss import maskNLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a386ced2-f4d4-4dcf-affa-3c945f37d993",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "SOS = 1\n",
    "EOS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "194c8f8c-d33c-4dea-be0a-188f09e384ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = \"data/formatted_movie_lines.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f48ece6e-fc78-4896-a92d-5a5372ebda1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(datafile, encoding=\"utf-8\").read().strip().split(\"\\n\\n\")\n",
    "pairs = [[normalizeString(s) for s in pair.split(\"\\t\")] for pair in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f44059c9-aa7c-45bb-840d-d1d8c1fb2a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221282"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e0da060-2fd0-4983-be45-b5e443b9f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = vocabulary(\"Cornell Movie Dialogues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3de6516f-30bd-4dce-9823-83e55e697534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a bit more cleaning, so well remove any sentances that are too long\n",
    "def filterpair(p, max_length=10):\n",
    "    return len(p[0].split()) <= max_length and len(p[1].split()) <= max_length\n",
    "\n",
    "pairs = [pair for pair in pairs if filterpair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f30d31f1-fc1c-42ba-bb9a-77ebfc933d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75026"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f57a7aec-f702-4395-a343-935c3501863f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['that s because it s such a nice one .', 'forget french .'],\n",
       " ['there .', 'where ?'],\n",
       " ['you have my word . as a gentleman', 'you re sweet .'],\n",
       " ['hi .', 'looks like things worked out tonight huh ?'],\n",
       " ['you know chastity ?', 'i believe we share an art instructor'],\n",
       " ['have fun tonight ?', 'tons'],\n",
       " ['well no . . .', 'then that s all you had to say .'],\n",
       " ['then that s all you had to say .', 'but'],\n",
       " ['but', 'you always been this selfish ?'],\n",
       " ['do you listen to this crap ?', 'what crap ?']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "527b4732-1a82-45ee-b0e8-177eac5a3e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimRareWords(vocab, pairs, min_count = 3):\n",
    "    \n",
    "    vocab.trim(min_count=min_count)\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_ = pair[0]\n",
    "        reply_ = pair[1]\n",
    "        keepinput, keepreply = True, True\n",
    "        for word in input_.split(\" \"):\n",
    "            if word not in vocab.word2index:\n",
    "                keepinput = False\n",
    "                break\n",
    "        for word in reply_.split(\" \"):\n",
    "            if word not in vocab.word2index:\n",
    "                keepreply = False\n",
    "                break\n",
    "        if keepinput and keepreply:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(f\"After trimming kept {len(keep_pairs)} out of {len(pairs)}\")\n",
    "    \n",
    "    return keep_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "521d0b42-26ac-49f8-99c4-1e40002a379a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20093\n"
     ]
    }
   ],
   "source": [
    "for pair in pairs:\n",
    "    corpus.addSentance(pair[0])\n",
    "    corpus.addSentance(pair[1])\n",
    "\n",
    "print(corpus.num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dc3437a-390c-4a3e-a7d4-cef7ff5a482a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After trimming kept 62810 out of 75026\n"
     ]
    }
   ],
   "source": [
    "cleaned_pairs = trimRareWords(corpus, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdd9a599-27a2-4abc-8e1b-c992bf8cd43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['that s because it s such a nice one .', 'forget french .'],\n",
       " ['there .', 'where ?'],\n",
       " ['you have my word . as a gentleman', 'you re sweet .'],\n",
       " ['hi .', 'looks like things worked out tonight huh ?'],\n",
       " ['have fun tonight ?', 'tons'],\n",
       " ['well no . . .', 'then that s all you had to say .'],\n",
       " ['then that s all you had to say .', 'but'],\n",
       " ['but', 'you always been this selfish ?'],\n",
       " ['do you listen to this crap ?', 'what crap ?'],\n",
       " ['what good stuff ?', 'the real you .']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09ed4a0d-84d5-4763-8a9a-ca2fe05088ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexfromSentance(vocab:vocabulary, sentance:str):\n",
    "    return [vocab.word2index[word] for word in sentance.split(\" \")] + [EOS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0efea256-5019-4813-96b2-901bfae6e8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 11, 2]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.indexfromSentance(cleaned_pairs[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06bb3376-3832-4703-b0d9-66e313e75e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for pair in cleaned_pairs[:10]:\n",
    "    inputs.append(indexfromSentance(corpus, pair[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47d24fa2-bf40-4fe3-bea5-c6468356c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeropading(l, fillvalue = 0):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3c17280-c2a6-494e-9ff4-4f8c36e6037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarymatrix(l, value=0):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e36655cf-6e94-4971-8654-e995761bf2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = zeropading(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ecf0aade-3956-46b3-a35a-187e7175a61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 4, 5, 6, 4, 7, 8, 9, 10, 11, 2],\n",
       " [14, 11, 2],\n",
       " [17, 18, 19, 20, 11, 21, 8, 22, 2],\n",
       " [25, 11, 2],\n",
       " [18, 40, 31, 16, 2],\n",
       " [42, 43, 11, 11, 11, 2],\n",
       " [44, 3, 4, 45, 17, 46, 47, 48, 11, 2],\n",
       " [49, 2],\n",
       " [54, 17, 55, 47, 52, 56, 16, 2],\n",
       " [57, 58, 59, 16, 2]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fec31028-4e7e-4375-8846-2237127474e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 14, 17, 25, 18, 42, 44, 49, 54, 57),\n",
       " (4, 11, 18, 11, 40, 43, 3, 2, 17, 58),\n",
       " (5, 2, 19, 2, 31, 11, 4, 0, 55, 59),\n",
       " (6, 0, 20, 0, 16, 11, 45, 0, 47, 16),\n",
       " (4, 0, 11, 0, 2, 11, 17, 0, 52, 2),\n",
       " (7, 0, 21, 0, 0, 2, 46, 0, 56, 0),\n",
       " (8, 0, 8, 0, 0, 0, 47, 0, 16, 0),\n",
       " (9, 0, 22, 0, 0, 0, 48, 0, 2, 0),\n",
       " (10, 0, 2, 0, 0, 0, 11, 0, 0, 0),\n",
       " (11, 0, 0, 0, 0, 0, 2, 0, 0, 0),\n",
       " (2, 0, 0, 0, 0, 0, 0, 0, 0, 0)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8bfcb70-71e4-4548-9dd0-dc720c9352a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = binarymatrix(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d1bf33b-4fff-417b-be94-c661d506eed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 0, 1, 1],\n",
       " [1, 0, 1, 0, 1, 1, 1, 0, 1, 1],\n",
       " [1, 0, 1, 0, 1, 1, 1, 0, 1, 1],\n",
       " [1, 0, 1, 0, 0, 1, 1, 0, 1, 0],\n",
       " [1, 0, 1, 0, 0, 0, 1, 0, 1, 0],\n",
       " [1, 0, 1, 0, 0, 0, 1, 0, 1, 0],\n",
       " [1, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e05ccf0-f8a4-48b1-802a-8f511aa09fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns padded input sequence tensor as well as tensor of lengths for each of the padded seq in the batc\n",
    "def inputVar(l:list, vocab:vocabulary):\n",
    "    indexes_batch = [vocab.indexfromSentance(sentance) for sentance in l]\n",
    "    lengths = torch.tensor([len(index_array) for index_array in indexes_batch])\n",
    "    padlist = zeropading(indexes_batch)\n",
    "    padvar = torch.LongTensor(padlist)\n",
    "    return padvar, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35df8b76-8349-4a6b-8722-b17ae19f4be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns padded target sequence tensor, padding mask and maax target length\n",
    "def outputVar(l:list, vocab:vocabulary):\n",
    "    indexes_batch = [vocab.indexfromSentance(sentance) for sentance in l]\n",
    "    max_target_len = max([len(index_array) for index_array in indexes_batch])\n",
    "    padlist = zeropading(indexes_batch)\n",
    "    mask = binarymatrix(padlist)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padvar = torch.LongTensor(padlist)\n",
    "    return padvar, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9983aa17-2a38-4f94-bcfe-bf516ef47169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepares the data for training for a given batch of pairs\n",
    "def batch2traindata(vocab, pair_batch):\n",
    "    #Sort the question answers pairs in descending order\n",
    "    pair_batch.sort(key=lambda x:len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, vocab)\n",
    "    output, mask, max_target_len = outputVar(output_batch, vocab)\n",
    "    return inp, lengths, output, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5c2d509-369a-4e4e-a5b4-55d2c9bee942",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation of preprocessing steps\n",
    "batch_size = 5\n",
    "input_seq, lengths, target_seq, target_mask, max_target_length = batch2traindata(corpus, [random.choice(cleaned_pairs) for _ in range(batch_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a01ceb19-616e-4992-a735-2b57bffb7003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 153,  113,   34,  550, 1483],\n",
      "        [  34,   34,  108, 6394,   16],\n",
      "        [ 101,   67,  285,   16,    2],\n",
      "        [ 102,  882,    6,    2,    0],\n",
      "        [ 307, 1114,  158,    0,    0],\n",
      "        [  82,  225,   11,    0,    0],\n",
      "        [  60,   16,    2,    0,    0],\n",
      "        [ 246,    2,    0,    0,    0],\n",
      "        [  11,    0,    0,    0,    0],\n",
      "        [   2,    0,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "print(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b39daa5-24b2-4f3e-baf1-ac9e625353b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10,  8,  7,  4,  3])\n"
     ]
    }
   ],
   "source": [
    "print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ad0da604-0c9b-4783-a62b-437d9e34136e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 860,  344,  327,   66,  266],\n",
      "        [8058, 1184, 1544, 3183,   32],\n",
      "        [ 640,   11, 2398,   67,   11],\n",
      "        [  11,   17,   27, 6263,  143],\n",
      "        [   2,  543,   93,   73,   10],\n",
      "        [   0,  183,   11,    2,   16],\n",
      "        [   0,  522,    2,    0,    2],\n",
      "        [   0,   47,    0,    0,    0],\n",
      "        [   0, 1704,    0,    0,    0],\n",
      "        [   0,   11,    0,    0,    0],\n",
      "        [   0,    2,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "print(target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a719f60a-6d6a-42e8-be88-db42edc41ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 0, 1],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "print(target_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f65e55af-3c3c-4fae-b2f2-80608b4a7769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(max_target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d8010e9a-4b3a-4839-aece-432e3aefb6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = torch.nn.GRU(input_size = 5, hidden_size = 3, num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ce60aa9d-7d75-44f5-83de-46f86c634738",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c66f2455-e0eb-4245-b344-b301528dd6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7606, -0.6435, -0.1769, -1.2964, -0.9328],\n",
       "         [-0.5316, -0.5640,  0.0882,  1.3835,  0.3539],\n",
       "         [ 1.8324, -0.2292, -0.8020, -1.4747, -0.0541],\n",
       "         [-0.6323, -0.7169, -0.2268,  1.9690,  0.1143],\n",
       "         [ 0.6190,  0.8411, -2.2479,  0.2791, -0.6485]],\n",
       "\n",
       "        [[-0.5636, -0.2744,  1.0614,  1.6762, -0.2682],\n",
       "         [-1.8209,  0.5697, -0.6093, -1.2058,  0.9373],\n",
       "         [-0.3847, -0.2412, -0.3489,  0.3419, -1.9268],\n",
       "         [ 0.0416,  0.3064, -0.1255, -0.9199, -0.6506],\n",
       "         [-0.2896, -1.4179,  1.7530,  0.1146,  0.2073]],\n",
       "\n",
       "        [[-1.7934,  1.5147,  1.4379,  0.9670,  1.0614],\n",
       "         [-0.4330,  0.3590, -0.3347,  0.6064,  0.1310],\n",
       "         [ 0.1455, -0.1465,  0.2352, -0.0569,  0.2094],\n",
       "         [-0.5671, -0.6823,  0.7690, -1.1584,  0.2873],\n",
       "         [ 1.7295, -1.0829, -0.5514,  2.2988, -0.1104]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "20f194ea-bcfc-47c6-ba14-8abe02af79c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(6, 7) #(batch_size, seq_len or max words per bach)\n",
    "lengths = [7,7,6,5,4,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "593de5af-cc71-4310-b016-2d102d1d5d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 7, 6, 5, 4, 2]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "18ebc41a-3424-4774-8a59-f9db0ff551da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6346, 0.4711, 0.9906, 0.7849, 0.7462, 0.5860, 0.5907],\n",
       "        [0.8719, 0.9433, 0.5214, 0.8487, 0.1704, 0.8424, 0.5195],\n",
       "        [0.4170, 0.1870, 0.0618, 0.3893, 0.1823, 0.8014, 0.0000],\n",
       "        [0.3333, 0.0549, 0.3940, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3459, 0.1796, 0.0064, 0.5855, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3830, 0.3656, 0.8027, 0.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e0c1e668-9dd8-4581-8d2d-286a4a576fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.nn.utils.rnn.pack_padded_sequence(a, lengths, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "00cbb49b-c305-455c-a837-e494fdadb241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6346, 0.8719, 0.4170, 0.3333, 0.3459, 0.3830, 0.4711, 0.9433, 0.1870,\n",
       "        0.0549, 0.1796, 0.3656, 0.9906, 0.5214, 0.0618, 0.3940, 0.0064, 0.7849,\n",
       "        0.8487, 0.3893, 0.0000, 0.5855, 0.7462, 0.1704, 0.1823, 0.0000, 0.5860,\n",
       "        0.8424, 0.8014, 0.5907, 0.5195])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0c2fad74-4b0e-4036-bcdc-bde8468788dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 6, 5, 5, 4, 3, 2])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cff46d3a-62e0-4247-810c-33e6a0d6231f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6346, 0.8719, 0.4170, 0.3333, 0.3459, 0.3830])\n",
      "tensor([0.4711, 0.9433, 0.1870, 0.0549, 0.1796, 0.3656])\n",
      "tensor([0.9906, 0.5214, 0.0618, 0.3940, 0.0064])\n",
      "tensor([0.7849, 0.8487, 0.3893, 0.0000, 0.5855])\n",
      "tensor([0.7462, 0.1704, 0.1823, 0.0000])\n",
      "tensor([0.5860, 0.8424, 0.8014])\n",
      "tensor([0.5907, 0.5195])\n"
     ]
    }
   ],
   "source": [
    "cutoff = 0\n",
    "for i in targets[1]:\n",
    "    print(targets[0][cutoff:cutoff+i])\n",
    "    cutoff += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7fa4ab30-1d6c-47e7-8827-03d6561aab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.rand(5,5) #(batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0332749d-2e2d-48b0-80e3-72405d43ada6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5633, 0.5129, 0.8349, 0.5787, 0.7113],\n",
       "        [0.7678, 0.4710, 0.0760, 0.6818, 0.1525],\n",
       "        [0.0555, 0.3180, 0.1772, 0.6100, 0.9287],\n",
       "        [0.7592, 0.5092, 0.7764, 0.2982, 0.7805],\n",
       "        [0.8202, 0.4984, 0.6043, 0.1431, 0.4397]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so each value in any row is basically the encoded word\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "824179fc-6ef2-47fe-aa24-fcfe60877a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.nn.functional.softmax(b, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e7971551-495f-4c77-86bf-47fadf3d9b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1839, 0.1749, 0.2413, 0.1867, 0.2132],\n",
       "        [0.2701, 0.2008, 0.1352, 0.2479, 0.1460],\n",
       "        [0.1322, 0.1719, 0.1493, 0.2301, 0.3165],\n",
       "        [0.2248, 0.1751, 0.2287, 0.1418, 0.2296],\n",
       "        [0.2687, 0.1947, 0.2165, 0.1365, 0.1836]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a2214727-a084-4d89-b937-b1584877289b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bd0a3a96-4e07-4ec6-ace8-085f9287cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Negetive Log Liklehood Loss\n",
    "def maskNLLLoss(decoder_out, target, mask, device):\n",
    "    ntotal = mask.sum()\n",
    "    target = target.view(-1,1)\n",
    "    #Decoder output shape: (batch_size, vocab_size), targets_size = (batch_size,1)\n",
    "    gathered_tensor = torch.gather(decoder_out,1,target)\n",
    "    cross_entropy = -torch.log(gathered_tensor)\n",
    "    loss = cross_entropy.masked_select(mask)\n",
    "    loss = loss.mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, ntotal.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415112ed-c032-40ac-8e77-261527259caf",
   "metadata": {},
   "source": [
    "### Demo Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "94482de9-8124-4c57-b67b-38b62df64e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Array Shape: torch.Size([11, 5])\n",
      "Lengths Shape: torch.Size([5])\n",
      "Target Array Shape: torch.Size([11, 5])\n",
      "Mask Shape: torch.Size([11, 5])\n",
      "Max Target Length: 11\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "batches = batch2traindata(corpus, [random.choice(cleaned_pairs) for _ in range(batch_size)])\n",
    "input_array, lengths, target_array, target_mask, max_target_len = batches\n",
    "print(f\"Input Array Shape: {input_array.shape}\")\n",
    "print(f\"Lengths Shape: {lengths.shape}\")\n",
    "print(f\"Target Array Shape: {target_array.shape}\")\n",
    "print(f\"Mask Shape: {target_mask.shape}\")\n",
    "print(f\"Max Target Length: {max_target_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5989eb3d-bf1d-47a1-9598-9ca7082e7821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining trainiing parameters\n",
    "hidden_size = 500\n",
    "enocder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "attn_model = 'dot'\n",
    "embedding = torch.nn.Embedding(corpus.num_words, hidden_size)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e13f0bfe-ac03-4209-bfa2-1f8d570edd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(9030, 500)\n",
       "  (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (gru): GRU(500, 500, num_layers=2, dropout=0.1)\n",
       "  (concat): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (attention): Attention()\n",
       "  (out): Linear(in_features=500, out_features=9030, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(hidden_size, embedding, enocder_n_layers, dropout)\n",
    "decoder = Decoder(attn_model, embedding, hidden_size, corpus.num_words, decoder_n_layers, dropout)\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9488d215-8a6f-43de-8c53-8d6a81bb6ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.train()\n",
    "decoder.train()\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), 0.0001)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), 0.0001)\n",
    "encoder_optimizer.zero_grad()\n",
    "decoder_optimizer.zero_grad()\n",
    "input_array = input_array.to(device)\n",
    "lengths = lengths.to(\"cpu\")\n",
    "target_array = targer_array.to(device)\n",
    "target_mask = target_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4f13dd13-f33e-45de-92b0-225b044186eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, encoder_hidden = encoder(input_array, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1cd091f0-8b4f-4c33-858b-8858c0129ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output Shape: torch.Size([11, 5, 500])\n",
      "Encoder Hidden Shapes: torch.Size([4, 5, 500])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Encoder Output Shape: {encoder_outputs.shape}\")\n",
    "print(f\"Encoder Hidden Shapes: {encoder_hidden.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "30cf001e-38a9-4522-878b-972c5456202c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "decoder_input = torch.LongTensor([[1 for _ in range(batch_size)]])\n",
    "decoder_input = decoder_input.to(device)\n",
    "print(decoder_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0c65bd24-bdf7-44ad-9847-fb1c59ea866f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 500])\n"
     ]
    }
   ],
   "source": [
    "#Setting the inital decoder hidden states as the encoder last hidden states obviously making it so that it fits using the n_layers\n",
    "decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "print(decoder_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "75583005-b924-4644-95c3-fa80317ade71",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "loss_array = []\n",
    "n_totals = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "815caf0d-abd2-4ed7-b3ca-5b88cbe565ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Output Shape: torch.Size([5, 9030])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "The target variable at the current timestep before reshaping: tensor([443,  50,  43,   3,  57], device='cuda:0')\n",
      "The Shape of the target variable before reshaping torch.Size([11, 5])\n",
      "Target variable shape after reshaping torch.Size([1, 5])\n",
      "Mask at the current timestep tensor([1, 1, 1, 1, 1], device='cuda:0', dtype=torch.uint8)\n",
      "Mask at the current timestep shape torch.Size([5])\n",
      "Mask Loss 9.112746238708496\n",
      "Total Items: 5\n",
      "Loss Array [45.56373119354248]\n",
      "Total Items 5\n",
      "Returned Loss 9.112746238708496\n",
      "----------------------End of Timestep--------------------------------------\n",
      "Decoder Output Shape: torch.Size([5, 9030])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "The target variable at the current timestep before reshaping: tensor([ 93, 425,   3, 363,  54], device='cuda:0')\n",
      "The Shape of the target variable before reshaping torch.Size([11, 5])\n",
      "Target variable shape after reshaping torch.Size([1, 5])\n",
      "Mask at the current timestep tensor([1, 1, 1, 1, 1], device='cuda:0', dtype=torch.uint8)\n",
      "Mask at the current timestep shape torch.Size([5])\n",
      "Mask Loss 9.141641616821289\n",
      "Total Items: 5\n",
      "Loss Array [45.56373119354248, 45.708208084106445]\n",
      "Total Items 10\n",
      "Returned Loss 9.127193927764893\n",
      "----------------------End of Timestep--------------------------------------\n",
      "Decoder Output Shape: torch.Size([5, 9030])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "The target variable at the current timestep before reshaping: tensor([11, 11,  4,  6, 17], device='cuda:0')\n",
      "The Shape of the target variable before reshaping torch.Size([11, 5])\n",
      "Target variable shape after reshaping torch.Size([1, 5])\n",
      "Mask at the current timestep tensor([1, 1, 1, 1, 1], device='cuda:0', dtype=torch.uint8)\n",
      "Mask at the current timestep shape torch.Size([5])\n",
      "Mask Loss 9.127211570739746\n",
      "Total Items: 5\n",
      "Loss Array [45.56373119354248, 45.708208084106445, 45.63605785369873]\n",
      "Total Items 15\n",
      "Returned Loss 9.12719980875651\n",
      "----------------------End of Timestep--------------------------------------\n",
      "Decoder Output Shape: torch.Size([5, 9030])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "The target variable at the current timestep before reshaping: tensor([  2,   2,  74,  11, 288], device='cuda:0')\n",
      "The Shape of the target variable before reshaping torch.Size([11, 5])\n",
      "Target variable shape after reshaping torch.Size([1, 5])\n",
      "Mask at the current timestep tensor([1, 1, 1, 0, 1], device='cuda:0', dtype=torch.uint8)\n",
      "Mask at the current timestep shape torch.Size([5])\n",
      "Mask Loss 9.085604667663574\n",
      "Total Items: 4\n",
      "Loss Array [45.56373119354248, 45.708208084106445, 45.63605785369873, 36.3424186706543]\n",
      "Total Items 19\n",
      "Returned Loss 9.11844293694747\n",
      "----------------------End of Timestep--------------------------------------\n",
      "Decoder Output Shape: torch.Size([5, 9030])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "The target variable at the current timestep before reshaping: tensor([ 0,  0,  6, 11, 83], device='cuda:0')\n",
      "The Shape of the target variable before reshaping torch.Size([11, 5])\n",
      "Target variable shape after reshaping torch.Size([1, 5])\n",
      "Mask at the current timestep tensor([0, 1, 1, 0, 1], device='cuda:0', dtype=torch.uint8)\n",
      "Mask at the current timestep shape torch.Size([5])\n",
      "Mask Loss 9.127809524536133\n",
      "Total Items: 3\n",
      "Loss Array [45.56373119354248, 45.708208084106445, 45.63605785369873, 36.3424186706543, 27.3834285736084]\n",
      "Total Items 22\n",
      "Returned Loss 9.119720198891379\n",
      "----------------------End of Timestep--------------------------------------\n",
      "Decoder Output Shape: torch.Size([5, 9030])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "The target variable at the current timestep before reshaping: tensor([ 0,  0, 11, 11,  4], device='cuda:0')\n",
      "The Shape of the target variable before reshaping torch.Size([11, 5])\n",
      "Target variable shape after reshaping torch.Size([1, 5])\n",
      "Mask at the current timestep tensor([0, 1, 1, 0, 1], device='cuda:0', dtype=torch.uint8)\n",
      "Mask at the current timestep shape torch.Size([5])\n",
      "Mask Loss 9.152486801147461\n",
      "Total Items: 3\n",
      "Loss Array [45.56373119354248, 45.708208084106445, 45.63605785369873, 36.3424186706543, 27.3834285736084, 27.457460403442383]\n",
      "Total Items 25\n",
      "Returned Loss 9.12365219116211\n",
      "----------------------End of Timestep--------------------------------------\n",
      "Decoder Output Shape: torch.Size([5, 9030])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "The target variable at the current timestep before reshaping: tensor([   0,    0, 1573,    2,  718], device='cuda:0')\n",
      "The Shape of the target variable before reshaping torch.Size([11, 5])\n",
      "Target variable shape after reshaping torch.Size([1, 5])\n",
      "Mask at the current timestep tensor([0, 1, 1, 0, 1], device='cuda:0', dtype=torch.uint8)\n",
      "Mask at the current timestep shape torch.Size([5])\n",
      "Mask Loss 9.120951652526855\n",
      "Total Items: 3\n",
      "Loss Array [45.56373119354248, 45.708208084106445, 45.63605785369873, 36.3424186706543, 27.3834285736084, 27.457460403442383, 27.362854957580566]\n",
      "Total Items 28\n",
      "Returned Loss 9.123362847736903\n",
      "----------------------End of Timestep--------------------------------------\n",
      "Decoder Output Shape: torch.Size([5, 9030])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "The target variable at the current timestep before reshaping: tensor([   0,    0, 7263,    0,   16], device='cuda:0')\n",
      "The Shape of the target variable before reshaping torch.Size([11, 5])\n",
      "Target variable shape after reshaping torch.Size([1, 5])\n",
      "Mask at the current timestep tensor([0, 1, 1, 0, 1], device='cuda:0', dtype=torch.uint8)\n",
      "Mask at the current timestep shape torch.Size([5])\n",
      "Mask Loss 9.128575325012207\n",
      "Total Items: 3\n",
      "Loss Array [45.56373119354248, 45.708208084106445, 45.63605785369873, 36.3424186706543, 27.3834285736084, 27.457460403442383, 27.362854957580566, 27.38572597503662]\n",
      "Total Items 31\n",
      "Returned Loss 9.12386728102161\n",
      "----------------------End of Timestep--------------------------------------\n",
      "Decoder Output Shape: torch.Size([5, 9030])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "The target variable at the current timestep before reshaping: tensor([   0,    0, 3239,    0,    2], device='cuda:0')\n",
      "The Shape of the target variable before reshaping torch.Size([11, 5])\n",
      "Target variable shape after reshaping torch.Size([1, 5])\n",
      "Mask at the current timestep tensor([0, 1, 1, 0, 1], device='cuda:0', dtype=torch.uint8)\n",
      "Mask at the current timestep shape torch.Size([5])\n",
      "Mask Loss 9.104715347290039\n",
      "Total Items: 3\n",
      "Loss Array [45.56373119354248, 45.708208084106445, 45.63605785369873, 36.3424186706543, 27.3834285736084, 27.457460403442383, 27.362854957580566, 27.38572597503662, 27.314146041870117]\n",
      "Total Items 34\n",
      "Returned Loss 9.122177404515883\n",
      "----------------------End of Timestep--------------------------------------\n",
      "Decoder Output Shape: torch.Size([5, 9030])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "The target variable at the current timestep before reshaping: tensor([ 0,  0, 11,  0,  0], device='cuda:0')\n",
      "The Shape of the target variable before reshaping torch.Size([11, 5])\n",
      "Target variable shape after reshaping torch.Size([1, 5])\n",
      "Mask at the current timestep tensor([0, 0, 0, 0, 1], device='cuda:0', dtype=torch.uint8)\n",
      "Mask at the current timestep shape torch.Size([5])\n",
      "Mask Loss 9.125085830688477\n",
      "Total Items: 1\n",
      "Loss Array [45.56373119354248, 45.708208084106445, 45.63605785369873, 36.3424186706543, 27.3834285736084, 27.457460403442383, 27.362854957580566, 27.38572597503662, 27.314146041870117, 9.125085830688477]\n",
      "Total Items 35\n",
      "Returned Loss 9.12226050240653\n",
      "----------------------End of Timestep--------------------------------------\n",
      "Decoder Output Shape: torch.Size([5, 9030])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "The target variable at the current timestep before reshaping: tensor([0, 0, 2, 0, 0], device='cuda:0')\n",
      "The Shape of the target variable before reshaping torch.Size([11, 5])\n",
      "Target variable shape after reshaping torch.Size([1, 5])\n",
      "Mask at the current timestep tensor([0, 0, 0, 0, 1], device='cuda:0', dtype=torch.uint8)\n",
      "Mask at the current timestep shape torch.Size([5])\n",
      "Mask Loss 9.120224952697754\n",
      "Total Items: 1\n",
      "Loss Array [45.56373119354248, 45.708208084106445, 45.63605785369873, 36.3424186706543, 27.3834285736084, 27.457460403442383, 27.362854957580566, 27.38572597503662, 27.314146041870117, 9.125085830688477, 9.120224952697754]\n",
      "Total Items 36\n",
      "Returned Loss 9.122203959359062\n",
      "----------------------End of Timestep--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Assuming we are using teacher forcing\n",
    "for t in range(max_target_len):\n",
    "    # print(decoder_input.device, decoder_hidden.device, encoder_outputs.device)\n",
    "    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "    print(f\"Decoder Output Shape: {decoder_output.shape}\")\n",
    "    print(f\"Decoder Hidden Shape: {decoder_hidden.shape}\")\n",
    "    #Teacher Forcing: New Input is current target\n",
    "    decoder_input = target_array[t].view(1,-1)\n",
    "    print(f\"The target variable at the current timestep before reshaping:\", target_array[t])\n",
    "    print(f\"The Shape of the target variable before reshaping {target_array.shape}\")\n",
    "    print(f\"Target variable shape after reshaping {decoder_input.shape}\")\n",
    "    #Calculate and Accumulate loss\n",
    "    print(f\"Mask at the current timestep {target_mask[t]}\")\n",
    "    print(f\"Mask at the current timestep shape {target_mask[t].shape}\")\n",
    "    mask_loss, total_items = maskNLLLoss(decoder_output, target_array[t], target_mask[t].bool(), device)\n",
    "    print(f\"Mask Loss {mask_loss}\")\n",
    "    print(f\"Total Items: {total_items}\")\n",
    "    loss += mask_loss\n",
    "    loss_array.append(mask_loss.item() * total_items)\n",
    "    print(f\"Loss Array {loss_array}\")\n",
    "    n_totals += total_items\n",
    "    print(f\"Total Items {n_totals}\")\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    returned_loss = sum(loss_array) / n_totals\n",
    "    print(f\"Returned Loss {returned_loss}\")\n",
    "    print(\"----------------------End of Timestep--------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2c664165-a185-4705-aca2-6d7b69d89d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62810"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c24da87d-8b61-44d5-b54e-631f127529bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import ChatDataset\n",
    "chatdata = ChatDataset(cleaned_pairs, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b460e4d8-5c86-44ba-bcec-8ab12b06c59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['that s because it s such a nice one .', 'forget french .']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatdata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cb8c2dc4-8864-48d2-ac49-93dfe0055107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "371c00e8-a11a-4618-94c2-7f86a3dd4f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return batch2traindata(corpus, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ab4108d7-9072-4d30-9ebb-ee227c8cdeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(chatdata, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5088f36e-9b65-438f-bad9-69dda62ed69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Array Shape: torch.Size([11, 32])\n",
      "Lengths Shape: torch.Size([32])\n",
      "Target Array Shape: torch.Size([11, 32])\n",
      "Mask Shape: torch.Size([11, 32])\n",
      "Max Target Length: 11\n",
      "tensor([[  83,   87,   34,   15,  104,  393, 1748, 3885,   72,  174,   49,   17,\n",
      "          438,   34,   14,   28,   57,  281,   57,    6,  459,   85,   15,    8,\n",
      "           34,   43,  174,  344, 8301,  930,   57, 8974],\n",
      "        [1127,    4,  303,  141,  477, 8073,   11,   34,  276,    8,   17, 1170,\n",
      "           96,  125,    4,  111,   82,  733,  111,    4,   17,    8,  111,  184,\n",
      "          101, 5276,  416,  974,   11,   73,   16, 8975],\n",
      "        [  11,   52,   47,   52,  958,   11,   11,  116,  102,  188,  101,  117,\n",
      "          846,  509,  361,  709, 6110,    4,   17,  524, 1375, 1273,   60,   63,\n",
      "          102,   63, 3224,  425,    2,    2,    2,    2],\n",
      "        [  11,  167, 1308, 5596, 1148,   11,   11,   17,  183,  183,  102,  522,\n",
      "            4,  198,    8,   63,    4,  361,  306,   14,   93,  253,  685,  903,\n",
      "           33,  950,   16,   11,    0,    0,    0,    0],\n",
      "        [  11,   34,   57,   83,  243,   11,  101,   47,   60,   27,   35,   47,\n",
      "          188,   17,  343, 7055, 4673,   14,   16,   11,   16,   11,   16,   11,\n",
      "           11,   16,    2,    2,    0,    0,    0,    0],\n",
      "        [  11,   50,  204,    4, 1114,   54,  102,   11, 8213,    3,    6,  282,\n",
      "           11,   11,   11,   11,   16,   11,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    0,    0,    0,    0,    0,    0],\n",
      "        [  11,  393,   17,  233,    6,   17,  553,   11,   11,   16,   11,   11,\n",
      "            2,    2,    2,    2,    2,    2,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [  11,  199,   71,   82,  402, 1329,   11,   11,    2,    2,    2,    2,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [  98,  654, 1339,   16,   11,   16,    2,    2,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [  11,   16,   16,    2,    2,    2,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   2,    2,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]])\n",
      "---------------------ENDOFBATCH------------------------------\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    input_array, lengths, target_array, target_mask, max_target_len = batch\n",
    "    print(f\"Input Array Shape: {input_array.shape}\")\n",
    "    print(f\"Lengths Shape: {lengths.shape}\")\n",
    "    print(f\"Target Array Shape: {target_array.shape}\")\n",
    "    print(f\"Mask Shape: {target_mask.shape}\")\n",
    "    print(f\"Max Target Length: {max_target_len}\")\n",
    "    print(input_array)\n",
    "    print(\"---------------------ENDOFBATCH------------------------------\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1aebd804-0650-477f-bb28-1e4ec02ede1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = decoder_output.topk(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0c34f6be-98fb-4418-a6a6-a8f98139e733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a6e0b711-92fd-460b-b2c1-05b040279d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0001],\n",
       "        [0.0002],\n",
       "        [0.0001],\n",
       "        [0.0001],\n",
       "        [0.0001]], device='cuda:0', grad_fn=<TopkBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6b54626c-c1c7-449b-bcf4-2769af406c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1553e-04, 1.1294e-04, 1.0690e-04,  ..., 1.1395e-04, 1.0120e-04,\n",
       "         1.1549e-04],\n",
       "        [1.0311e-04, 1.0790e-04, 1.0737e-04,  ..., 1.1787e-04, 9.2372e-05,\n",
       "         1.1191e-04],\n",
       "        [1.0052e-04, 9.9224e-05, 1.0639e-04,  ..., 1.0647e-04, 1.0601e-04,\n",
       "         1.0701e-04],\n",
       "        [1.1131e-04, 1.1383e-04, 1.0399e-04,  ..., 1.1313e-04, 9.5457e-05,\n",
       "         1.0915e-04],\n",
       "        [1.1124e-04, 1.1287e-04, 1.1108e-04,  ..., 1.0921e-04, 9.8871e-05,\n",
       "         1.1760e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d9137d26-1ab6-4897-a5d6-76500d373552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1963"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9cccce-354b-47c2-9a9e-e290cf8b693d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llms]",
   "language": "python",
   "name": "conda-env-llms-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
